import os
# os.environ["PYTORCH_ENABLE_CUDA_FALLBACK"] = "1"
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"


# å»ºä¸€å€‹ä½ æœ‰æ¬Šé™çš„ temp è³‡æ–™å¤¾
# os.environ["TMPDIR"] = "/home/rd/workspace/LLM_test/DiscordChatBot/tmp"
# os.makedirs(os.environ["TMPDIR"], exist_ok=True)

import random
import torch
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionImg2ImgPipeline, StableDiffusionControlNetImg2ImgPipeline, ControlNetModel,DPMSolverMultistepScheduler, EulerAncestralDiscreteScheduler, AutoencoderKL

# --- åƒæ•¸è¨­å®š ---
#model_dir = "src/Models/SD/BaseModels/anypastelAnythingV45_anypastelAnythingV45"  # ä½ çš„ä¸»æ¨¡å‹è³‡æ–™å¤¾ï¼ˆå·²è½‰æˆ diffusers æ ¼å¼ï¼‰
#model_dir = "src/Models/SD/BaseModels/meinaunreal_v5"
#model_dir = "src/Models/SD/BaseModels/hadrianDelice_deliceV20"
#model_dir = "src/Models/SD/BaseModels/meinamix_v12Final"
model_dir = "src/Models/SD/BaseModels/qchanAnimeMix_v40"
#vae_path = "DiscordChatBot/src/Models/converted-orangemixvaeReupload_v10"  # VAE æ¨¡å‹ï¼ˆéœ€è¦è½‰æ›æ ¼å¼ï¼‰
#vae_path = "src/Models/SD/VAE/klF8Anime2VAE_klF8Anime2VAE"
vae_path = ""

def get_image_resized_info(image_path: str, bigger:bool=False, smaller:bool=False) -> tuple[str , Image.Image]:
    img = Image.open(image_path)
    w, h = img.size
    if bigger:
        resized_img = img.resize((w*2,h*2), Image.BICUBIC)
        return img.filename.split('/')[-1], resized_img
    if smaller:
        resized_img = img.resize((w//2,h//2), Image.LANCZOS)
        return img.filename.split('/')[-1], resized_img
    # --- è¨ˆç®—æ–°çš„å¤§å° ---
    if w == h:
        new_size = (512, 512)
    elif w > h:
        new_w = 768
        new_h = int(h * (768 / w))
        new_size = (new_w, new_h)
    else:
        new_h = 768
        new_w = int(w * (768 / h))
        if new_w > 512:
            new_w = 512
        new_size = (new_w, new_h)
    
    # BICUBICé©åˆæ”¾å¤§åœ–åƒ
    # LANCZOSé©åˆç¸®å°åœ–åƒ
    resized_img = img.resize(new_size, Image.LANCZOS)

    return img.filename.split('/')[-1], resized_img

# --- è¼‰å…¥åŸåœ–é€²è¡Œç¸®æ”¾ ---
original_image = Image.open("src/inputs/cosplay.jpg")
info = get_image_resized_info("src/inputs/cosplay.jpg", True)
init_image = info[1].convert("RGB")
init_image_name = info[0]

def get_canny_img(image_name: str):
    canny_path = f'src/inputs/{image_name}-canny.png'
    if os.path.exists(canny_path):
        return canny_path, Image.open(canny_path)
    else: 
        # --- åˆ©ç”¨ OpenCV ç”Ÿæˆ Canny é‚Šç·£åœ– ---
        # è½‰æ›æˆ numpy é™£åˆ—ï¼Œä¸¦è½‰æˆç°éš
        np_image = np.array(init_image)
        gray = cv2.cvtColor(np_image, cv2.COLOR_RGB2GRAY)
        # ä½¿ç”¨ Canny åµæ¸¬é‚Šç·£ï¼Œé€™è£¡åƒæ•¸å¯ä¾éœ€æ±‚èª¿æ•´
        edges = cv2.Canny(gray, threshold1=50, threshold2=150)
        # å°‡é‚Šç·£åœ–è½‰å› RGB PIL Image
        control_image = Image.fromarray(cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB))
        control_image.save(canny_path)
        return canny_path, control_image

canny_path, control_image = get_canny_img(init_image_name)

_ , control_image = get_image_resized_info(canny_path)

# --- è¼‰å…¥ ControlNet æ¨¡å‹ï¼ˆä»¥ Canny ç‚ºä¾‹ï¼‰ ---
# è«‹ç¢ºä¿å·²ä¸‹è¼‰ "lllyasviel/sd-controlnet-canny"
controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/sd-controlnet-canny", torch_dtype=torch.float32
)

# --- å»ºç«‹ ControlNet + img2img ç®¡ç·š ---
pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
    # "runwayml/stable-diffusion-v1-5",
    model_dir,
    controlnet=controlnet,
    torch_dtype=torch.float32,
    safety_checker=None
)


# # âœ… è¼‰å…¥ VAEï¼ˆç”¨ from_pretrainedï¼‰
# vae = AutoencoderKL.from_pretrained(
#     "DiscordChatBot/src/Models/converted-orangemixvaeReupload_v10",
#     torch_dtype=torch.float32
# )

# # ğŸ”— æŒ‡æ´¾çµ¦ç®¡ç·š
# pipe.vae = vae

#pipe.vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-mse", torch_dtype=torch.float32).to("mps")
if vae_path:
    pipe.vae = AutoencoderKL.from_pretrained(vae_path, torch_dtype=torch.float32).to("mps")

pipe = pipe.to("mps")  # æˆ– "mps" æ ¹æ“šä½ çš„è¨­å‚™
pipe.enable_attention_slicing()

# --- è¼‰å…¥ LoRA æ¨¡å‹ ---
# pipe.load_lora_weights("src/temp/animeoutlineV4_16.safetensors", adapter_name="animeoutline")
#pipe.load_lora_weights("src/temp/OutlineSuperV9.safetensors", adapter_name="OutlineSuperV9")

# pipe.load_lora_weights("DiscordChatBot/src/Models/Lora/best/adapter_model.safetensors", adapter_name="ghibli")

# pipe.set_adapters(["ghibli"])

#pipe.load_lora_weights("DiscordChatBot/src/Models/Lora/best/adapter_model.safetensors", adapter_name="ghibli")
# pipe.load_lora_weights("DiscordChatBot/src/Models/Lora/best/adapter_model.safetensors")

# pipe.load_lora_weights("DiscordChatBot/src/Models/animetarotV51.safetensors", 
#                         adapter_name="animetarotV51")
# lora_weights = { "animetarotV51": 1 }
# pipe.set_adapters(list(lora_weights.keys()), list(lora_weights.values()))

# pipe.load_lora_weights("src/Models/SD/Loras/ghibli_style_offset.safetensors", 
#                         adapter_name="ghibli_style")
# lora_weights = { "ghibli_style": 1 }
# pipe.set_adapters(list(lora_weights.keys()), list(lora_weights.values()))

pipe.load_textual_inversion("src/Models/SD/Embeddings/easynegative.safetensors", token="easynegative", mean_resizing=False)
pipe.load_textual_inversion("src/Models/SD/Embeddings/badhandv4.pt", token="badhandv4", mean_resizing=False)

# --- è¨­å®šæ¡æ¨£å™¨ ---
# pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)
# ä½¿ç”¨ DPM++ 2M Karras æ¡æ¨£å™¨
pipe.scheduler = DPMSolverMultistepScheduler.from_config(
    pipe.scheduler.config,
    use_karras_sigmas=True  # âœ… é–‹å•Ÿ Karras åˆ†å¸ƒ
)
negative_prompt = "easynegative, badhandv4"
prompt = "(((masterpiece))),(((bestquality))),1girl,beautiful eyes"
#seed = 2245550560
seed =  random.randint(0, 2**32 - 1)
guidance_scale = 7
steps = 25
generator = torch.manual_seed(seed)

# --- ç”Ÿæˆåœ–ç‰‡ ---
# æ³¨æ„é€™è£¡é™¤äº†åŸåœ– (image) åƒæ•¸å¤–ï¼Œå¤šäº†ä¸€å€‹ control_image åƒæ•¸
result = pipe(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=get_image_resized_info("src/inputs/cosplay.jpg", smaller=True)[1],
    control_image=get_image_resized_info(canny_path, smaller=True)[1],
    strength=0.5,         # èª¿æ•´é¢¨æ ¼è½‰æ›ç¨‹åº¦ï¼Œå»ºè­°ä¿ç•™åŸåœ–çµæ§‹å¯èª¿ä½
    guidance_scale=guidance_scale,
    num_inference_steps=steps,
    generator=generator
).images[0]

result_path = f"src/results-new/{init_image_name}-result-{seed}.png"
result.save(result_path)
print(result_path)
#Image.open(result_path).show()